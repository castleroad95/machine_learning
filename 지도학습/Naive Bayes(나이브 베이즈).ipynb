{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 베이즈 정리\n",
    "\n",
    "- 조건부 확률로 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연속적인 특성으로 분류기 훈련\n",
    "\n",
    "- 베이즈 이론은 새로운 정보 P(A|B)와 사전 확률P(A)가 주어졌을 때 어떤 사건이 일어날 확률\n",
    "\n",
    "- 직관적인 방법 사용\n",
    "- 작은 양의 데이터에서 사용 가능\n",
    "- 훈련과 예측에 비용이 적음\n",
    "- 환경이 바뀌어도 자주 안정적인 결과 만듬\n",
    "\n",
    "- 나이브 베이즈 분류기는 각 특성과 특성의 가능도가 독립적이라 가정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "iris = datasets.load_iris() # 데이터 로드\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "classifer = GaussianNB() # 가우시안 나이브 베이지 객체 생성\n",
    "model = classifer.fit(features, target) # 모델 훈련\n",
    "new_observation = [[ 4, 4, 4, 0.4]] #New Sample Data\n",
    "model.predict(new_observation) # 클래스 예측\n",
    "\n",
    "# 각 클래스별 사전 확률을 지정한 가우시안 나이브 베이즈 객체 생성\n",
    "clf = GaussianNB(priors=[0.25, 0.25, 0.5])\n",
    "model = classifer.fit(features, target) # 모델 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이산적인 카운트 특성으로 분류기 훈련\n",
    "\n",
    "- 다항 나이브 베이즈가 가장 많이 사용되는 경우중 하나는 BoW나 tf-idf방식을 사용한 텍스트 분류이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text_data = np.array(['I love Brazil. Brazil!','Brazil is best','Germany beats both'])\n",
    "\n",
    "count = CountVectorizer() # BoW(bag of words) 생성\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "features = bag_of_words.toarray() # 특성 행렬 생성\n",
    "target = np.array([0,0,1]) # 타깃 벡터 생성\n",
    "\n",
    "#MultinomialNB를 사용해 두 클래스(brazil과 germany)에 대한 사전 확률을 지정하여 모델을 훈련\n",
    "# 각 클래스별 사전 확률을 지정한 다항 나이브 베이즈 객체 생성\n",
    "classifer = MultinomialNB(class_prior=[0.25, 0.5])\n",
    "model = classifer.fit(features, target) # 모델 훈련\n",
    "#New Sample Data\n",
    "new_observation = [[0, 0, 0, 1, 0, 1, 0]] \n",
    "# 새로운 샘플의 클래스 예측\n",
    "model.predict(new_observation) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이진 특성으로 나이브 베이지 분류기 훈련\n",
    "\n",
    "- 모든 특성이 두 종류의 값만 발생할 수 있는 이진 특성이라고 가정\n",
    "- 텍스트 분류에 많이 사용\n",
    "- 사전 확률 지정시 class_prior 매개변수에 클래스별 사전 확률 담은 리스트 전달\n",
    "- 균등분포를 사용하려면 fit_prior=False로 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# 세 개의 이진 특성 만듬\n",
    "features = np.random.randint(2, size=(100, 3))\n",
    "\n",
    "# 이진 타깃 벡터 만듬\n",
    "target = np.random.randint(2, size=(100, 1)).ravel()\n",
    "\n",
    "# 각 클래스별 사전 확률을 지정하여 베르누이 나이브 베이즈 객체 만듬\n",
    "classifer = BernoulliNB(class_prior=[0.25, 0.5])\n",
    "model = classifer.fit(features, target) # 모델 훈련\n",
    "\n",
    "model_uniform_prior = BernoulliNB(class_prior=None, fit_prior=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예측 확률 보정\n",
    "\n",
    "- 타깃 클래스에 대한 예측 확률의 순위는 유효하지만 예측 확률이 0 또는 1에 극단적으로 가까워지는 경향이 있음\n",
    "- 반환된 예측 확률은 k-폴드의 평균\n",
    "- CalibratedClassifierCV는 플랫의 시그모이드 모델과 등위회귀 두개의 보정 방법 지원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.31859969, 0.63663466, 0.04476565]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "iris = datasets.load_iris() # 데이터 로드\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "classifer = GaussianNB() # 가우시안 나이브 베이즈 객체 생성\n",
    "# 시그모이드 보정을 사용해 보정 교차 검증을 만듬\n",
    "classifer_sigmoid = CalibratedClassifierCV(classifer, cv=2, method='sigmoid')\n",
    "classifer_sigmoid.fit(features, target) # 확률을 보정\n",
    "new_observation = [[ 2.6, 2.6, 2.6, 0.4]] #New Sample Data\n",
    "classifer_sigmoid.predict_proba(new_observation) # 보정된 확률을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.31859969, 0.63663466, 0.04476565]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#가우시안 나이브 베이즈를 훈련하고 클래스 확률을 예측\n",
    "classifer.fit(features, target).predict_proba(new_observation)\n",
    "classifer_sigmoid.predict_proba(new_observation) # 보정된 확률을 확인"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
